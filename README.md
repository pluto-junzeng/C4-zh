# C4-zh

随着预训练模型发展，中文预训练模型对于学术界和工业界更加重要，我们从C4 以及其他公开的数据集中中文自然语言数据集，从而构建大规模高质量的中文预训练语料


#### 目标
构建100G的高质量中文无监督语料，来源新闻，百科，评论等


#### 数据来源

- 已有数据大小

| 数据来源 | 数据规模                       | 大小 | 数据来源链接 | 下载链接（自行构建或清洗）|
| -------- | ------------------------------ | ---- | ---- | ---- |
| 搜狐新闻 | 2008~2019 共计600w条 ，未出重 |   21G    |  [2012](https://www.sogou.com/labs/resource/list_news.php) [2014-2016](https://github.com/brightmart/nlp_chinese_corpus) [2009-2016](https://www.jianshu.com/p/370d3e67a18f)    |
| 百度知道     |      60万条              |  3G |     |
|  百度搜索       |     60万条                           |   3G   |      |
| 新浪新闻 | 2008~2019滚动新闻  共计 10w条              |   2G   |      |
| 百度百科    |      2012年百度百科 ,400w词条             |  22G  |      |
|百度百科|2019年百度百科，500w词条|50G|baike.baidu.com|不提供下载，[下载教程](https://blog.csdn.net/u013741019/article/details/102882731)|
| 清华新闻       |    86万条                            |  4G    |      |
| 维基中文       |    50万条                            |  2G    |      |
| 微信公众号文章| 未知|3G|[来源](https://github.com/nonamestreet/weixin_public_corpus)||


#### Reference

大规模中文自然语言处理语料 Large Scale Chinese Corpus for NLP  https://github.com/brightmart/nlp_chinese_corpus

